{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceType":"competition","sourceId":101849,"databundleVersionId":13093295},{"sourceType":"competition","sourceId":99552,"databundleVersionId":13441085}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# NeurIPS : notebook by pragnyanramtha","metadata":{}},{"cell_type":"code","source":"# ===================================================================\n# 1.1: Project Setup and Dependencies\n# File: setup.py\n# ===================================================================\nimport os\nimport logging\nimport torch\nimport warnings\n\ndef setup_project_environment():\n    \"\"\"\n    Creates directories, installs packages, and configures logging.\n    \"\"\"\n    # --- Directory Creation ---\n    print(\"Creating project directories...\")\n    directories = ['data', 'notebooks', 'src/data_processing', 'src/models', \n                   'src/evaluation', 'src/utils', 'results/models', 'results/submissions']\n    for directory in directories:\n        os.makedirs(directory, exist_ok=True)\n    \n    # --- Package Installation ---\n    # In a real environment, you would run this in your terminal.\n    # We will list the command here for completeness.\n    print(\"\\n---\")\n    print(\"Run the following command in your terminal to install dependencies:\")\n    pip_install_command = (\"pip install pandas numpy scikit-learn xgboost catboost lightgbm \"\n                           \"tabpfn torch torchvision torchaudio matplotlib seaborn pyarrow fastparquet\")\n    print(f\"$ {pip_install_command}\")\n    print(\"---\\n\")\n\n    # --- CUDA Configuration ---\n    print(\"Checking for CUDA support...\")\n    is_cuda_available = torch.cuda.is_available()\n    print(f\"CUDA Available: {is_cuda_available}\")\n    if not is_cuda_available:\n        print(\"WARNING: CUDA not found. Training will be on CPU.\")\n    \n    # --- Logging and Warnings Configuration ---\n    print(\"Configuring logging...\")\n    logging.basicConfig(level=logging.INFO, \n                        format='%(asctime)s - %(levelname)s - %(message)s',\n                        filename='project_log.log',\n                        filemode='w')\n    \n    # Suppress common warnings for cleaner output\n    warnings.filterwarnings('ignore', category=FutureWarning)\n    \n    print(\"\\nProject setup complete.\")\n    logging.info(\"Project environment set up successfully.\")\n\n# Execute the setup\nsetup_project_environment()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T10:01:19.863779Z","iopub.execute_input":"2025-09-10T10:01:19.864585Z","iopub.status.idle":"2025-09-10T10:01:19.872963Z","shell.execute_reply.started":"2025-09-10T10:01:19.864558Z","shell.execute_reply":"2025-09-10T10:01:19.872120Z"}},"outputs":[{"name":"stdout","text":"Creating project directories...\n\n---\nRun the following command in your terminal to install dependencies:\n$ pip install pandas numpy scikit-learn xgboost catboost lightgbm tabpfn torch torchvision torchaudio matplotlib seaborn pyarrow fastparquet\n---\n\nChecking for CUDA support...\nCUDA Available: True\nConfiguring logging...\n\nProject setup complete.\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"## Core Data Loading and Preprocessing\n\nThis section contains the functions responsible for interacting with the raw data. We begin by creating a robust data loader that handles the large parquet files, performs the critical ADC conversion to restore the data's dynamic range, and includes error handling. We then implement functions to apply the various calibration frames, such as dark subtraction and flat-field correction, to clean the instrumental signatures from the signal.","metadata":{}},{"cell_type":"code","source":"# ===================================================================\n# 2.1 & 2.2: Data Loading and Calibration\n# File: src/data_processing/loader.py\n# ===================================================================\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\n\n\n# ===================================================================\n# File: src/data_processing/loader_gpu.py\n# Description: GPU-accelerated data processing using CuPy.\n# ===================================================================\nimport cupy as cp  # Import CuPy\nimport pandas as pd\nfrom pathlib import Path\nimport logging\n\n# We can reuse the original ADC loader\n# from src.data_processing.loader import load_adc_info\n\n\ndef load_adc_info(data_path):\n    \"\"\"Loads ADC conversion parameters.\"\"\"\n    return pd.read_csv(Path(data_path) / 'adc_info.csv').iloc[0]\n\ndef load_and_move_to_gpu(file_path, instrument):\n    \"\"\"Loads a parquet file and immediately moves its data to the GPU.\"\"\"\n    try:\n        df = pd.read_parquet(file_path)\n        # Move data to GPU as a CuPy array\n        raw_signal_gpu = cp.asarray(df.to_numpy()) \n        \n        if instrument == 'FGS1':\n            return raw_signal_gpu.reshape(-1, 32, 32)\n        elif instrument == 'AIRS-CH0':\n            return raw_signal_gpu.reshape(-1, 32, 356)\n        else:\n            raise ValueError(\"Unknown instrument\")\n            \n    except Exception as e:\n        logging.error(f\"GPU Load Failed for {file_path}: {e}\")\n        return None\n\ndef apply_adc_conversion_gpu(signal_gpu, adc_params):\n    \"\"\"Applies ADC conversion on the GPU.\"\"\"\n    gain = adc_params['gain']\n    offset = adc_params['offset']\n    # All arithmetic is now done on the GPU\n    return (signal_gpu / gain + offset).astype(cp.float64)\n\ndef apply_calibrations_gpu(signal_gpu, calib_data_gpu):\n    \"\"\"Applies a simplified calibration pipeline on the GPU.\"\"\"\n    processed_signal_gpu = signal_gpu\n    if 'dark' in calib_data_gpu:\n        processed_signal_gpu = processed_signal_gpu - calib_data_gpu['dark']\n    if 'flat' in calib_data_gpu:\n        flat_gpu = calib_data_gpu['flat']\n        # Avoid division by zero on the GPU\n        flat_gpu[flat_gpu == 0] = 1\n        processed_signal_gpu = processed_signal_gpu / flat_gpu\n    return processed_signal_gpu\n\ndef create_light_curve_gpu(signal_data_gpu):\n    \"\"\"Creates a 1D light curve on the GPU.\"\"\"\n    if signal_data_gpu.ndim != 3:\n        return cp.array([])\n    # cp.sum is the CuPy equivalent of np.sum\n    return cp.sum(signal_data_gpu, axis=(1, 2))\n\ndef extract_temporal_features_gpu(light_curve_gpu):\n    \"\"\"Extracts basic statistical features on the GPU.\"\"\"\n    if light_curve_gpu.size == 0:\n        # Return values must be moved to CPU for the final dictionary\n        return {'mean': 0.0, 'std': 0.0, 'min': 0.0, 'max': 0.0}\n    \n    # cp.asnumpy() moves the final scalar result from GPU to CPU\n    return {\n        'mean': cp.asnumpy(cp.mean(light_curve_gpu)),\n        'std': cp.asnumpy(cp.std(light_curve_gpu)),\n        'min': cp.asnumpy(cp.min(light_curve_gpu)),\n        'max': cp.asnumpy(cp.max(light_curve_gpu))\n    }\n\ndef process_planet_gpu(planet_dir, adc_params, star_info_df):\n    \"\"\"\n    Main GPU processing function for a single planet.\n    Orchestrates loading, calibration, and feature extraction on the GPU.\n    \"\"\"\n    planet_path = Path(planet_dir)\n    planet_id = int(planet_path.name)\n    features = {'planet_id': planet_id}\n\n    for instrument in ['FGS1', 'AIRS-CH0']:\n        visit_files = sorted(list(planet_path.glob(f'{instrument}_signal_*.parquet')))\n        if not visit_files:\n            # Handle cases where an instrument file might be missing\n            temp_features = extract_temporal_features_gpu(cp.array([]))\n            for key, val in temp_features.items():\n                features[f'{instrument}_{key}'] = val\n            continue\n\n        all_visits_gpu = []\n        for visit_file in visit_files:\n            visit_id = visit_file.stem.split('_')[-1]\n            \n            # Load raw signal directly to GPU\n            signal_gpu = load_and_move_to_gpu(visit_file, instrument)\n            if signal_gpu is None: continue\n\n            # Apply ADC on GPU\n            signal_gpu = apply_adc_conversion_gpu(signal_gpu, adc_params)\n            \n            # Load and apply calibrations on GPU\n            calib_path = planet_path / f\"{instrument}_calibration_{visit_id}\"\n            calib_data_gpu = {}\n            for calib_type in ['dark', 'flat']: # Simplified for example\n                calib_file = calib_path / f\"{calib_type}.parquet\"\n                if calib_file.exists():\n                    # Load and move calibration frame to GPU\n                    calib_data_gpu[calib_type] = cp.asarray(pd.read_parquet(calib_file).to_numpy())\n            \n            calibrated_signal_gpu = apply_calibrations_gpu(signal_gpu, calib_data_gpu)\n            all_visits_gpu.append(calibrated_signal_gpu)\n\n        # Combine visits and extract features\n        if all_visits_gpu:\n            full_signal_gpu = cp.concatenate(all_visits_gpu, axis=0)\n            light_curve_gpu = create_light_curve_gpu(full_signal_gpu)\n            temp_features = extract_temporal_features_gpu(light_curve_gpu)\n        else:\n            temp_features = extract_temporal_features_gpu(cp.array([]))\n        \n        for key, val in temp_features.items():\n            features[f'{instrument}_{key}'] = val\n\n    # Add stellar features (this is a CPU operation)\n    stellar_params = star_info_df[star_info_df['planet_id'] == planet_id].iloc[0]\n    features.update(stellar_params.to_dict())\n    \n    return features\n\n# --- 2.1: Data Loading Utilities ---\n\n# ===================================================================\n# File: src/models/boosting_gpu.py\n# ===================================================================\nimport xgboost as xgb\nimport lightgbm as lgb\nimport catboost as cb\nfrom sklearn.model_selection import train_test_split\nimport sklearn.multioutput\nimport numpy as np\n\n# Assuming the evaluation metric function `calculate_weighted_gll` is in another file\n# from src.evaluation.metrics import calculate_weighted_gll\n\ndef train_and_evaluate_boosting_gpu(X, y):\n    \"\"\"\n    Trains and evaluates GPU-accelerated XGBoost, LightGBM, and CatBoost models.\n    \"\"\"\n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n    \n    print(\"\\n--- Training GPU-Accelerated Boosting Models ---\")\n    \n    # --- 1. XGBoost ---\n    print(\"\\nTraining XGBoost...\")\n    # Use MultiOutputRegressor wrapper for multi-target prediction\n    xgb_model = sklearn.multioutput.MultiOutputRegressor(\n        xgb.XGBRegressor(tree_method='gpu_hist', objective='reg:squarederror', n_estimators=1000)\n    )\n    xgb_model.fit(X_train, y_train) # XGBoost can handle numpy arrays directly\n    \n    y_pred_mean_xgb = xgb_model.predict(X_val)\n    # Simple uncertainty: std of training residuals\n    unc_xgb = np.std(y_train - xgb_model.predict(X_train), axis=0)\n    score_xgb = calculate_weighted_gll(y_val, y_pred_mean_xgb, unc_xgb)\n    print(f\"XGBoost Validation GLL: {score_xgb:.4f}\")\n\n    # --- 2. LightGBM ---\n    print(\"\\nTraining LightGBM...\")\n    lgb_model = sklearn.multioutput.MultiOutputRegressor(\n        lgb.LGBMRegressor(device='gpu', objective='regression', n_estimators=1000)\n    )\n    lgb_model.fit(X_train, y_train)\n    \n    y_pred_mean_lgb = lgb_model.predict(X_val)\n    unc_lgb = np.std(y_train - lgb_model.predict(X_train), axis=0)\n    score_lgb = calculate_weighted_gll(y_val,data_pred_mean_lgb, unc_lgb)\n    print(f\"LightGBM Validation GLL: {score_lgb:.4f}\")\n\n    # --- 3. CatBoost ---\n    print(\"\\nTraining CatBoost...\")\n    # CatBoost's Multi-output support is native but can be tricky. Using the wrapper is safer.\n    cat_model = sklearn.multioutput.MultiOutputRegressor(\n        cb.CatBoostRegressor(task_type='GPU', iterations=1000, verbose=0)\n    )\n    cat_model.fit(X_train, y_train)\n    \n    y_pred_mean_cat = cat_model.predict(X_val)\n    unc_cat = np.std(y_train - cat_model.predict(X_train), axis=0)\n    score_cat = calculate_weighted_gll(y_val, y_pred_mean_cat, unc_cat)\n    print(f\"CatBoost Validation GLL: {score_cat:.4f}\")\n    \n    return {'xgb': xgb_model, 'lgb': lgb_model, 'cat': cat_model}\n\n\n\n# ===================================================================\n# File: src/models/pytorch_nn.py\n# ===================================================================\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom tqdm import tqdm\n\n# --- 2.1: PyTorch Dataset and DataLoader ---\n\ndef create_dataloaders(X, y, batch_size=64):\n    \"\"\"Creates PyTorch DataLoaders for training and validation sets.\"\"\"\n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n    \n    # Convert numpy arrays to PyTorch tensors\n    X_train_t = torch.tensor(X_train, dtype=torch.float32)\n    y_train_t = torch.tensor(y_train, dtype=torch.float32)\n    X_val_t = torch.tensor(X_val, dtype=torch.float32)\n    y_val_t = torch.tensor(y_val, dtype=torch.float32)\n    \n    train_dataset = TensorDataset(X_train_t, y_train_t)\n    val_dataset = TensorDataset(X_val_t, y_val_t)\n    \n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n    \n    return train_loader, val_loader\n\n# --- 2.2: PyTorch Model Architecture ---\n\nclass SimpleMLP(nn.Module):\n    \"\"\"A simple Multi-Layer Perceptron for regression.\"\"\"\n    def __init__(self, input_dim, output_dim):\n        super(SimpleMLP, self).__init__()\n        self.network = nn.Sequential(\n            nn.Linear(input_dim, 128),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(128, 256),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(256, output_dim)\n        )\n        \n    def forward(self, x):\n        return self.network(x)\n\n# --- 2.3: PyTorch Training and Evaluation Loop ---\n\ndef train_and_evaluate_pytorch_model(X, y, epochs=20):\n    \"\"\"\n    Main function to orchestrate the training and evaluation of a PyTorch model.\n    \"\"\"\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"\\n--- Training PyTorch Model on {device} ---\")\n    \n    # Create DataLoaders\n    train_loader, val_loader = create_dataloaders(X, y)\n    \n    # Initialize model, loss, and optimizer\n    input_dim = X.shape[1]\n    output_dim = y.shape[1]\n    model = SimpleMLP(input_dim, output_dim).to(device)\n    criterion = nn.MSELoss() # A common loss for regression\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n    \n    # Training loop\n    for epoch in range(epochs):\n        model.train()\n        total_loss = 0\n        for batch_X, batch_y in train_loader:\n            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n            \n            optimizer.zero_grad()\n            outputs = model(batch_X)\n            loss = criterion(outputs, batch_y)\n            loss.backward()\n            optimizer.step()\n            \n            total_loss += loss.item()\n        \n        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {total_loss / len(train_loader):.6f}\")\n\n    # Evaluation\n    model.eval()\n    all_preds = []\n    all_true = []\n    with torch.no_grad():\n        for batch_X, batch_y in val_loader:\n            batch_X = batch_X.to(device)\n            outputs = model(batch_X)\n            all_preds.append(outputs.cpu().numpy())\n            all_true.append(batch_y.numpy())\n            \n    y_pred_mean_nn = np.concatenate(all_preds)\n    y_val = np.concatenate(all_true)\n    \n    # Simple uncertainty estimation\n    unc_nn = np.std(y - model(torch.tensor(X, dtype=torch.float32).to(device)).detach().cpu().numpy(), axis=0)\n    score_nn = calculate_weighted_gll(y_val, y_pred_mean_nn, unc_nn)\n    print(f\"PyTorch MLP Validation GLL: {score_nn:.4f}\")\n    \n    return model\n# --- 2.3: Multi-Visit Combination ---\n\ndef process_all_planet_visits(planet_path, adc_params):\n    \"\"\"\n    Loads all data for a single planet, handles multiple visits, \n    and applies calibrations.\n    \"\"\"\n    planet_path = Path(planet_path)\n    processed_data = {'FGS1': [], 'AIRS-CH0': []}\n\n    for instrument in ['FGS1', 'AIRS-CH0']:\n        visit_files = sorted(list(planet_path.glob(f'{instrument}_signal_*.parquet')))\n        \n        for visit_file in visit_files:\n            visit_id = visit_file.stem.split('_')[-1]\n            \n            # Load signal data\n            signal_data = load_signal_data(visit_file, adc_params, instrument)\n            if signal_data is None: continue\n            \n            # Load corresponding calibration data\n            calib_data = load_calibration_files(planet_path, instrument, visit_id)\n            \n            # Apply calibrations\n            calibrated_signal = apply_calibrations(signal_data, calib_data)\n            processed_data[instrument].append(calibrated_signal)\n            \n    # Combine visits by concatenating along the time axis\n    for instrument in processed_data:\n        if processed_data[instrument]:\n            processed_data[instrument] = np.concatenate(processed_data[instrument], axis=0)\n        else:\n            processed_data[instrument] = np.array([])\n            \n    return processed_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T10:01:20.047285Z","iopub.execute_input":"2025-09-10T10:01:20.048083Z","iopub.status.idle":"2025-09-10T10:01:20.080809Z","shell.execute_reply.started":"2025-09-10T10:01:20.048031Z","shell.execute_reply":"2025-09-10T10:01:20.079907Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"## Comprehensive Feature Engineering Pipeline\n\nWith the data loaded and cleaned, we now focus on feature engineering. The strategy is to reduce the dimensionality of the vast time-series data into a compact and informative feature vector. We perform simple aperture photometry to create 1D light curves, extract basic statistical features from these light curves, and combine them with the scaled stellar parameters to form the final input for our models.","metadata":{}},{"cell_type":"code","source":"# ===================================================================\n# 3.1, 3.2 & 3.3: Feature Engineering\n# File: src/feature_engineering/builder.py\n# ===================================================================\nfrom sklearn.preprocessing import StandardScaler\n\ndef create_light_curve(signal_data):\n    \"\"\"\n    Creates a simplified 1D light curve from 3D signal data by summing\n    all pixel values for each timestamp. This is a basic form of photometry.\n    \"\"\"\n    if signal_data.ndim != 3:\n        return np.array([])\n    # Sum across the spatial dimensions (height and width)\n    return np.sum(signal_data, axis=(1, 2))\n\ndef extract_temporal_features(light_curve):\n    \"\"\"Extracts basic statistical features from a light curve.\"\"\"\n    if light_curve.size == 0:\n        return {'mean': 0, 'std': 0, 'min': 0, 'max': 0}\n    \n    return {\n        'mean': np.mean(light_curve),\n        'std': np.std(light_curve),\n        'min': np.min(light_curve),\n        'max': np.max(light_curve)\n    }\n\ndef get_stellar_features(planet_id, star_info_df):\n    \"\"\"Retrieves stellar parameters for a given planet_id.\"\"\"\n    return star_info_df[star_info_df['planet_id'] == planet_id].iloc[0]\n\ndef build_feature_vector(planet_id, all_calibrated_data, star_info_df):\n    \"\"\"\n    Builds a single feature vector for a planet by combining temporal\n    and stellar features.\n    \"\"\"\n    features = {'planet_id': planet_id}\n    \n    # Temporal features from light curves\n    for instrument in ['FGS1', 'AIRS-CH0']:\n        light_curve = create_light_curve(all_calibrated_data[instrument])\n        temp_features = extract_temporal_features(light_curve)\n        for key, val in temp_features.items():\n            features[f'{instrument}_{key}'] = val\n            \n    # Stellar features\n    stellar_params = get_stellar_features(planet_id, star_info_df)\n    features.update(stellar_params.to_dict())\n    \n    return features","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T10:01:20.082265Z","iopub.execute_input":"2025-09-10T10:01:20.082510Z","iopub.status.idle":"2025-09-10T10:01:20.103637Z","shell.execute_reply.started":"2025-09-10T10:01:20.082491Z","shell.execute_reply":"2025-09-10T10:01:20.102919Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"## Baseline Models and Evaluation\n\nThis section establishes our modeling and evaluation framework. We implement the competition-specific Gaussian Log-Likelihood (GLL) metric, ensuring the heavy FGS1 channel weight is correctly applied. We then create a function to train a simple baseline model, such as Ridge regression, and a validation function to score its performance using a standard train-test split.","metadata":{}},{"cell_type":"code","source":"# ===================================================================\n# 4.1 & 4.2: Baseline Models and Evaluation\n# File: src/models/baseline.py and src/evaluation/metrics.py\n# ===================================================================\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import make_scorer\nimport sklearn.multioutput\n\n# --- 4.2: Evaluation Framework ---\n\ndef gll_score_single(y_true, y_pred_mean, y_pred_unc):\n    \"\"\"Calculates the Gaussian Log-Likelihood for a single prediction.\"\"\"\n    return -0.5 * (np.log(2 * np.pi) + np.log(y_pred_unc**2) + ((y_true - y_pred_mean)**2) / (y_pred_unc**2))\n\ndef calculate_weighted_gll(y_true, y_pred_mean, y_pred_unc, fgs1_weight=57.846):\n    \"\"\"Calculates the final weighted GLL score for the competition.\"\"\"\n    # Ensure inputs are numpy arrays\n    y_true = np.asarray(y_true)\n    y_pred_mean = np.asarray(y_pred_mean)\n    y_pred_unc = np.asarray(y_pred_unc)\n    \n    scores = gll_score_single(y_true, y_pred_mean, y_pred_unc)\n    \n    # Apply weights\n    weights = np.ones(y_true.shape[1])\n    weights[0] = fgs1_weight  # First column is FGS1\n    \n    weighted_scores = scores * weights\n    return np.sum(weighted_scores)\n\n\n# --- 4.1: Simple Regression Baselines ---\n\ndef train_and_evaluate_baseline(X, y):\n    \"\"\"Trains a Ridge Regressor and evaluates it using the GLL score.\"\"\"\n    \n    # Split data\n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n    \n    print(f\"Training on {X_train.shape[0]} samples, validating on {X_val.shape[0]} samples.\")\n    \n    # Initialize and train a simple model\n    # We use a MultiOutputRegressor to predict all 283 wavelengths at once\n    model = sklearn.multioutput.MultiOutputRegressor(Ridge(alpha=1.0))\n    model.fit(X_train, y_train)\n    \n    # Make predictions\n    y_pred_mean = model.predict(X_val)\n    \n    # Estimate uncertainty: A simple approach is to use the standard deviation\n    # of the training residuals as a constant uncertainty for all predictions.\n    train_residuals = y_train - model.predict(X_train)\n    y_pred_unc = np.std(train_residuals, axis=0)\n    \n    # Evaluate\n    score = calculate_weighted_gll(y_val, y_pred_mean, y_pred_unc)\n    print(f\"Validation Weighted GLL Score: {score:.4f}\")\n    \n    return model, score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T10:01:20.104561Z","iopub.execute_input":"2025-09-10T10:01:20.104838Z","iopub.status.idle":"2025-09-10T10:01:20.123836Z","shell.execute_reply.started":"2025-09-10T10:01:20.104819Z","shell.execute_reply":"2025-09-10T10:01:20.123298Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"## Executing the Pipeline\n\nFinally, this is the main execution script. It orchestrates the entire process by calling the functions defined above in the correct sequence. It iterates through all training planets, applies the data processing and feature engineering steps, aggregates the results into a single dataset, and then trains and evaluates our baseline model.","metadata":{}},{"cell_type":"code","source":"# ===================================================================\n# File: main.py (Updated Version)\n# ===================================================================\nimport pandas as pd\nfrom tqdm import tqdm\nfrom pathlib import Path\nimport logging\n\n\ndef main_gpu():\n    \"\"\"Main function to run the GPU-accelerated pipeline.\"\"\"\n    BASE_PATH = Path('/kaggle/input/ariel-data-challenge-2025')\n    TRAIN_PATH = BASE_PATH / 'train'\n    \n    logging.info(\"Starting GPU-accelerated pipeline execution.\")\n    \n    # --- Load Metadata (on CPU) ---\n    print(\"Loading metadata...\")\n    adc_params = load_adc_info(BASE_PATH)\n    star_info_df = pd.read_csv(BASE_PATH / 'train_star_info.csv')\n    ground_truth_df = pd.read_csv(BASE_PATH / 'train.csv')\n    \n    # --- GPU-Accelerated Feature Engineering Loop ---\n    print(\"Building feature vectors for all training planets using CuPy...\")\n    planet_dirs = [d for d in TRAIN_PATH.iterdir() if d.is_dir()]\n    \n    all_features = []\n    # The main loop now calls the GPU processing function\n    for planet_dir in tqdm(planet_dirs, desc=\"GPU Processing Planets\"):\n        feature_vector = process_planet_gpu(planet_dir, adc_params, star_info_df)\n        all_features.append(feature_vector)\n    \n    feature_df = pd.DataFrame(all_features)\n    print(\"GPU feature engineering complete.\")\n    \n    # --- Model Training (unchanged from before) ---\n    print(\"Preparing data for model training...\")\n    merged_df = pd.merge(feature_df, ground_truth_df, on='planet_id', how='inner')\n    \n    feature_cols = [col for col in feature_df.columns if col != 'planet_id']\n    target_cols = [col for col in ground_truth_df.columns if col.startswith('wl_')]\n    \n    X = merged_df[feature_cols].values\n    y = merged_df[target_cols].values\n    \n    # It's good practice to scale features for NNs and some boosting models\n    from sklearn.preprocessing import StandardScaler\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X)\n    \n    # --- Run GPU-Accelerated Models ---\n    train_and_evaluate_boosting_gpu(X_scaled, y)\n    train_and_evaluate_pytorch_model(X_scaled, y)\n    \n    logging.info(\"GPU pipeline execution finished.\")\n\n# --- Run the main GPU pipeline ---\nif __name__ == '__main__':\n    # Ensure you have CuPy installed and a compatible GPU\n    try:\n        import cupy\n        print(f\"CuPy installation found. Running on GPU.\")\n        main_gpu()\n    except ImportError:\n        print(\"CuPy not found. Please install CuPy to run the GPU-accelerated pipeline.\")\n        # Optionally, you could fall back to the CPU main function here.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T10:01:20.125111Z","iopub.execute_input":"2025-09-10T10:01:20.125319Z","iopub.status.idle":"2025-09-10T10:01:23.188461Z","shell.execute_reply.started":"2025-09-10T10:01:20.125291Z","shell.execute_reply":"2025-09-10T10:01:23.187280Z"}},"outputs":[{"name":"stdout","text":"CuPy installation found. Running on GPU.\nLoading metadata...\nBuilding feature vectors for all training planets using CuPy...\n","output_type":"stream"},{"name":"stderr","text":"GPU Processing Planets:   0%|          | 0/1100 [00:01<?, ?it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'gain'","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/2035561926.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mcupy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"CuPy installation found. Running on GPU.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mmain_gpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CuPy not found. Please install CuPy to run the GPU-accelerated pipeline.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_36/2035561926.py\u001b[0m in \u001b[0;36mmain_gpu\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# The main loop now calls the GPU processing function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mplanet_dir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplanet_dirs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"GPU Processing Planets\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mfeature_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_planet_gpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplanet_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madc_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstar_info_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mall_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_36/2951676986.py\u001b[0m in \u001b[0;36mprocess_planet_gpu\u001b[0;34m(planet_dir, adc_params, star_info_df)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0;31m# Apply ADC on GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0msignal_gpu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapply_adc_conversion_gpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignal_gpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madc_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;31m# Load and apply calibrations on GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_36/2951676986.py\u001b[0m in \u001b[0;36mapply_adc_conversion_gpu\u001b[0;34m(signal_gpu, adc_params)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mapply_adc_conversion_gpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignal_gpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madc_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;34m\"\"\"Applies ADC conversion on the GPU.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mgain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madc_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gain'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0moffset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madc_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'offset'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;31m# All arithmetic is now done on the GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1120\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mkey_is_scalar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m         \u001b[0;31m# Convert generator to list before going through hashable part\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1236\u001b[0m         \u001b[0;31m# Similar to Index.get_value, but we do not fall back to positional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1237\u001b[0;31m         \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1239\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'gain'"],"ename":"KeyError","evalue":"'gain'","output_type":"error"}],"execution_count":15}]}
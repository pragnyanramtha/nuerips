{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":99552,"databundleVersionId":13441085,"sourceType":"competition"},{"sourceId":101849,"databundleVersionId":13093295,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# NeurIPS : notebook by pragnyanramtha","metadata":{}},{"cell_type":"code","source":"import os\nimport logging\nimport warnings\nfrom pathlib import Path\n\n# Data Handling & GPU Acceleration\nimport pandas as pd\nimport numpy as np\nimport cupy as cp\n\n# Machine Learning Models\nimport xgboost as xgb\nimport lightgbm as lgb\nimport catboost as cb\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Utilities\nimport sklearn.multioutput\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom tqdm.notebook import tqdm\n\ndef setup_project_environment():\n    \"\"\"Creates directories, and configures logging and CUDA.\"\"\"\n    print(\"--- Project Setup ---\")\n    directories = ['results/models', 'results/submissions']\n    for directory in directories:\n        os.makedirs(directory, exist_ok=True)\n\n    # Configure Logging\n    logging.basicConfig(level=logging.INFO, \n                        format='%(asctime)s - %(levelname)s - %(message)s',\n                        filename='project_log.log',\n                        filemode='w')\n    warnings.filterwarnings('ignore', category=FutureWarning)\n    \n    # Verify GPU and CuPy/PyTorch Support\n    try:\n        cp.cuda.runtime.getDeviceCount()\n        print(f\"CuPy found and can access GPU.\")\n    except cp.cuda.runtime.CUDARuntimeError as e:\n        print(f\"CuPy ERROR: {e}. Ensure CUDA toolkit is compatible.\")\n        \n    is_torch_cuda = torch.cuda.is_available()\n    print(f\"PyTorch CUDA Available: {is_torch_cuda}\")\n    if not is_torch_cuda:\n        print(\"WARNING: PyTorch cannot find CUDA. Training will be on CPU.\")\n    \n    print(\"Setup complete.\\n\")\n\n# Run the setup\nsetup_project_environment()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-11T04:31:40.141255Z","iopub.execute_input":"2025-09-11T04:31:40.141849Z","iopub.status.idle":"2025-09-11T04:31:40.149364Z","shell.execute_reply.started":"2025-09-11T04:31:40.141824Z","shell.execute_reply":"2025-09-11T04:31:40.148618Z"}},"outputs":[{"name":"stdout","text":"--- Project Setup ---\nCuPy found and can access GPU.\nPyTorch CUDA Available: True\nSetup complete.\n\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"## Core Data Loading and Preprocessing\n\nThis section contains the functions responsible for interacting with the raw data. We begin by creating a robust data loader that handles the large parquet files, performs the critical ADC conversion to restore the data's dynamic range, and includes error handling. We then implement functions to apply the various calibration frames, such as dark subtraction and flat-field correction, to clean the instrumental signatures from the signal.","metadata":{}},{"cell_type":"code","source":"def gll_score_single(y_true, y_pred_mean, y_pred_unc):\n    \"\"\"Calculates the Gaussian Log-Likelihood for a single prediction.\"\"\"\n    return -0.5 * (np.log(2 * np.pi) + np.log(y_pred_unc**2) + ((y_true - y_pred_mean)**2) / (y_pred_unc**2))\n\ndef calculate_weighted_gll(y_true, y_pred_mean, y_pred_unc, fgs1_weight=57.846):\n    \"\"\"Calculates the final weighted GLL score for the competition.\"\"\"\n    y_true = np.asarray(y_true)\n    y_pred_mean = np.asarray(y_pred_mean)\n    y_pred_unc = np.asarray(y_pred_unc)\n    \n    scores = gll_score_single(y_true, y_pred_mean, y_pred_unc)\n    \n    weights = np.ones(y_true.shape[1])\n    weights[0] = fgs1_weight\n    \n    weighted_scores = scores * weights\n    return np.sum(weighted_scores)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-11T04:31:40.150535Z","iopub.execute_input":"2025-09-11T04:31:40.150741Z","iopub.status.idle":"2025-09-11T04:31:40.173477Z","shell.execute_reply.started":"2025-09-11T04:31:40.150726Z","shell.execute_reply":"2025-09-11T04:31:40.172743Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"## Comprehensive Feature Engineering Pipeline\n\nWith the data loaded and cleaned, we now focus on feature engineering. The strategy is to reduce the dimensionality of the vast time-series data into a compact and informative feature vector. We perform simple aperture photometry to create 1D light curves, extract basic statistical features from these light curves, and combine them with the scaled stellar parameters to form the final input for our models.","metadata":{}},{"cell_type":"code","source":"def load_adc_info(data_path):\n    \"\"\"Loads ADC conversion parameters, handling instrument-specific keys.\"\"\"\n    adc_df = pd.read_csv(Path(data_path) / 'adc_info.csv')\n    return adc_df.iloc[0]\n\ndef load_and_move_to_gpu(file_path, instrument):\n    \"\"\"Loads a parquet file and immediately moves its data to the GPU.\"\"\"\n    df = pd.read_parquet(file_path)\n    raw_signal_gpu = cp.asarray(df.to_numpy())\n    shape = (-1, 32, 32) if instrument == 'FGS1' else (-1, 32, 356)\n    return raw_signal_gpu.reshape(shape)\n\ndef apply_adc_conversion_gpu(signal_gpu, adc_params, instrument):\n    \"\"\"Applies ADC conversion on the GPU using instrument-specific keys.\"\"\"\n    gain = adc_params[f\"{instrument}_adc_gain\"]\n    offset = adc_params[f\"{instrument}_adc_offset\"]\n    return (signal_gpu / gain + offset).astype(cp.float64)\n\ndef apply_calibrations_gpu(signal_gpu, calib_data_gpu):\n    \"\"\"Applies a simplified calibration pipeline on the GPU.\"\"\"\n    processed_signal_gpu = signal_gpu\n    if 'dark' in calib_data_gpu:\n        processed_signal_gpu -= calib_data_gpu['dark']\n    if 'flat' in calib_data_gpu:\n        flat_gpu = calib_data_gpu['flat']\n        flat_gpu[flat_gpu == 0] = 1\n        processed_signal_gpu /= flat_gpu\n    return processed_signal_gpu\n\ndef create_light_curve_gpu(signal_data_gpu):\n    \"\"\"Creates a 1D light curve on the GPU by summing pixel values.\"\"\"\n    return cp.sum(signal_data_gpu, axis=(1, 2)) if signal_data_gpu.ndim == 3 else cp.array([])\n\ndef extract_temporal_features_gpu(light_curve_gpu):\n    \"\"\"Extracts basic statistical features on the GPU and returns them to the CPU.\"\"\"\n    if light_curve_gpu.size == 0:\n        return {'mean': 0.0, 'std': 0.0, 'min': 0.0, 'max': 0.0}\n    return {\n        'mean': cp.asnumpy(cp.mean(light_curve_gpu)),\n        'std': cp.asnumpy(cp.std(light_curve_gpu)),\n        'min': cp.asnumpy(cp.min(light_curve_gpu)),\n        'max': cp.asnumpy(cp.max(light_curve_gpu))\n    }\n\ndef process_planet_gpu(planet_dir, adc_params, star_info_df):\n    \"\"\"Main GPU processing function for a single planet.\"\"\"\n    planet_path = Path(planet_dir)\n    planet_id = int(planet_path.name)\n    features = {'planet_id': planet_id}\n\n    for instrument in ['FGS1', 'AIRS-CH0']:\n        all_visits_gpu = []\n        for visit_file in sorted(planet_path.glob(f'{instrument}_signal_*.parquet')):\n            visit_id = visit_file.stem.split('_')[-1]\n            signal_gpu = load_and_move_to_gpu(visit_file, instrument)\n            signal_gpu = apply_adc_conversion_gpu(signal_gpu, adc_params, instrument)\n            \n            calib_path = planet_path / f\"{instrument}_calibration_{visit_id}\"\n            calib_data_gpu = {\n                ctype: cp.asarray(pd.read_parquet(f).to_numpy())\n                for ctype in ['dark', 'flat']\n                if (f := calib_path / f\"{ctype}.parquet\").exists()\n            }\n            \n            all_visits_gpu.append(apply_calibrations_gpu(signal_gpu, calib_data_gpu))\n\n        if all_visits_gpu:\n            full_signal_gpu = cp.concatenate(all_visits_gpu, axis=0)\n            light_curve_gpu = create_light_curve_gpu(full_signal_gpu)\n            temp_features = extract_temporal_features_gpu(light_curve_gpu)\n        else:\n            temp_features = extract_temporal_features_gpu(cp.array([]))\n        \n        for key, val in temp_features.items():\n            features[f'{instrument}_{key}'] = val\n\n    stellar_params = star_info_df.loc[star_info_df['planet_id'] == planet_id].iloc[0]\n    features.update(stellar_params.to_dict())\n    return features","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-11T04:31:40.174677Z","iopub.execute_input":"2025-09-11T04:31:40.174909Z","iopub.status.idle":"2025-09-11T04:31:40.194203Z","shell.execute_reply.started":"2025-09-11T04:31:40.174889Z","shell.execute_reply":"2025-09-11T04:31:40.193622Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"## Baseline Models and Evaluation\n\nThis section establishes our modeling and evaluation framework. We implement the competition-specific Gaussian Log-Likelihood (GLL) metric, ensuring the heavy FGS1 channel weight is correctly applied. We then create a function to train a simple baseline model, such as Ridge regression, and a validation function to score its performance using a standard train-test split.","metadata":{}},{"cell_type":"code","source":"def train_and_evaluate_boosting_gpu(X, y):\n    \"\"\"Trains and evaluates GPU-accelerated XGBoost, LightGBM, and CatBoost models.\"\"\"\n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n    print(\"\\n--- Training GPU-Accelerated Boosting Models ---\")\n\n    models = {\n        \"XGBoost\": xgb.XGBRegressor(tree_method='gpu_hist', n_estimators=1000),\n        \"LightGBM\": lgb.LGBMRegressor(device='gpu', n_estimators=1000),\n        \"CatBoost\": cb.CatBoostRegressor(task_type='GPU', iterations=1000, verbose=0)\n    }\n    \n    for name, model in models.items():\n        print(f\"\\nTraining {name}...\")\n        multi_output_model = sklearn.multioutput.MultiOutputRegressor(model)\n        multi_output_model.fit(X_train, y_train)\n        \n        y_pred_mean = multi_output_model.predict(X_val)\n        unc = np.std(y_train - multi_output_model.predict(X_train), axis=0)\n        score = calculate_weighted_gll(y_val, y_pred_mean, unc)\n        print(f\"{name} Validation GLL: {score:.4f}\")\n\nclass SimpleMLP(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super().__init__()\n        self.network = nn.Sequential(\n            nn.Linear(input_dim, 256), nn.ReLU(), nn.Dropout(0.3),\n            nn.Linear(256, 512), nn.ReLU(), nn.Dropout(0.3),\n            nn.Linear(512, output_dim)\n        )\n    def forward(self, x): return self.network(x)\n\ndef train_and_evaluate_pytorch_model(X, y, epochs=25, batch_size=128):\n    \"\"\"Orchestrates the training and evaluation of a PyTorch MLP.\"\"\"\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"\\n--- Training PyTorch Model on {device} ---\")\n\n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n    train_loader = DataLoader(TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32)), batch_size=batch_size, shuffle=True)\n    \n    model = SimpleMLP(X.shape[1], y.shape[1]).to(device)\n    criterion = nn.MSELoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n    \n    for epoch in range(epochs):\n        model.train()\n        for batch_X, batch_y in train_loader:\n            optimizer.zero_grad()\n            outputs = model(batch_X.to(device))\n            loss = criterion(outputs, batch_y.to(device))\n            loss.backward()\n            optimizer.step()\n        if (epoch + 1) % 5 == 0:\n            print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.6f}\")\n\n    model.eval()\n    with torch.no_grad():\n        y_pred_mean_nn = model(torch.tensor(X_val, dtype=torch.float32).to(device)).cpu().numpy()\n        train_preds = model(torch.tensor(X_train, dtype=torch.float32).to(device)).cpu().numpy()\n    \n    unc_nn = np.std(y_train - train_preds, axis=0)\n    score_nn = calculate_weighted_gll(y_val, y_pred_mean_nn, unc_nn)\n    print(f\"PyTorch MLP Validation GLL: {score_nn:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-11T04:31:40.194829Z","iopub.execute_input":"2025-09-11T04:31:40.195012Z","iopub.status.idle":"2025-09-11T04:31:40.211883Z","shell.execute_reply.started":"2025-09-11T04:31:40.194997Z","shell.execute_reply":"2025-09-11T04:31:40.211171Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"## Executing the Pipeline\n\nFinally, this is the main execution script. It orchestrates the entire process by calling the functions defined above in the correct sequence. It iterates through all training planets, applies the data processing and feature engineering steps, aggregates the results into a single dataset, and then trains and evaluates our baseline model.","metadata":{}},{"cell_type":"code","source":"BASE_PATH = Path('/kaggle/input/ariel-data-challenge-2025')\nTRAIN_PATH = BASE_PATH / 'train'\nPROCESSED_FEATURES_FILE = \"processed_features.parquet\"\n\n# --- PART 1: Run Preprocessing (only if needed) ---\nif not Path(PROCESSED_FEATURES_FILE).exists():\n    print(f\"'{PROCESSED_FEATURES_FILE}' not found. Running one-time preprocessing...\")\n    \n    adc_params = load_adc_info(BASE_PATH)\n    star_info_df = pd.read_csv(BASE_PATH / 'train_star_info.csv')\n    planet_dirs = [d for d in TRAIN_PATH.iterdir() if d.is_dir()]\n    \n    all_features = [\n        process_planet_gpu(p_dir, adc_params, star_info_df)\n        for p_dir in tqdm(planet_dirs, desc=\"GPU Preprocessing Planets\")\n    ]\n    \n    feature_df = pd.DataFrame(all_features)\n    feature_df.to_parquet(PROCESSED_FEATURES_FILE, index=False)\n    print(f\"\\nPreprocessing complete. Features saved to '{PROCESSED_FEATURES_FILE}'.\")\nelse:\n    print(f\"Found existing '{PROCESSED_FEATURES_FILE}'. Skipping preprocessing.\")\n\n# --- PART 2: Run Model Training ---\nprint(\"\\n--- Starting Model Training Stage ---\")\n\n# Load data\nfeature_df = pd.read_parquet(PROCESSED_FEATURES_FILE)\nground_truth_df = pd.read_csv(BASE_PATH / 'train.csv')\n\n# Merge and prepare for ML\nmerged_df = pd.merge(feature_df, ground_truth_df, on='planet_id', how='inner')\nfeature_cols = [col for col in feature_df.columns if col != 'planet_id']\ntarget_cols = [col for col in ground_truth_df.columns if col.startswith('wl_')]\n\nX = merged_df[feature_cols].values\ny = merged_df[target_cols].values\n\n# Scale features\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Train all models\ntrain_and_evaluate_boosting_gpu(X_scaled, y)\ntrain_and_evaluate_pytorch_model(X_scaled, y)\n\nprint(\"\\n--- Full Pipeline Finished ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-11T04:31:40.213109Z","iopub.execute_input":"2025-09-11T04:31:40.213266Z"}},"outputs":[{"name":"stdout","text":"'processed_features.parquet' not found. Running one-time preprocessing...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"GPU Preprocessing Planets:   0%|          | 0/1100 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90aacfb316f74ba8b04237a5b28438dc"}},"metadata":{}}],"execution_count":null}]}
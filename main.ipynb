{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceType":"competition","sourceId":101849,"databundleVersionId":13093295},{"sourceType":"competition","sourceId":99552,"databundleVersionId":13441085}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# NeurIPS : notebook by pragnyanramtha","metadata":{}},{"cell_type":"code","source":"# ===================================================================\n# 1.1: Project Setup and Dependencies\n# File: setup.py\n# ===================================================================\nimport os\nimport logging\nimport torch\nimport warnings\n\ndef setup_project_environment():\n    \"\"\"\n    Creates directories, installs packages, and configures logging.\n    \"\"\"\n    # --- Directory Creation ---\n    print(\"Creating project directories...\")\n    directories = ['data', 'notebooks', 'src/data_processing', 'src/models', \n                   'src/evaluation', 'src/utils', 'results/models', 'results/submissions']\n    for directory in directories:\n        os.makedirs(directory, exist_ok=True)\n    \n    # --- Package Installation ---\n    # In a real environment, you would run this in your terminal.\n    # We will list the command here for completeness.\n    print(\"\\n---\")\n    print(\"Run the following command in your terminal to install dependencies:\")\n    pip_install_command = (\"pip install pandas numpy scikit-learn xgboost catboost lightgbm \"\n                           \"tabpfn torch torchvision torchaudio matplotlib seaborn pyarrow fastparquet\")\n    print(f\"$ {pip_install_command}\")\n    print(\"---\\n\")\n\n    # --- CUDA Configuration ---\n    print(\"Checking for CUDA support...\")\n    is_cuda_available = torch.cuda.is_available()\n    print(f\"CUDA Available: {is_cuda_available}\")\n    if not is_cuda_available:\n        print(\"WARNING: CUDA not found. Training will be on CPU.\")\n    \n    # --- Logging and Warnings Configuration ---\n    print(\"Configuring logging...\")\n    logging.basicConfig(level=logging.INFO, \n                        format='%(asctime)s - %(levelname)s - %(message)s',\n                        filename='project_log.log',\n                        filemode='w')\n    \n    # Suppress common warnings for cleaner output\n    warnings.filterwarnings('ignore', category=FutureWarning)\n    \n    print(\"\\nProject setup complete.\")\n    logging.info(\"Project environment set up successfully.\")\n\n# Execute the setup\nsetup_project_environment()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T09:25:33.432735Z","iopub.execute_input":"2025-09-10T09:25:33.433078Z","iopub.status.idle":"2025-09-10T09:25:39.831203Z","shell.execute_reply.started":"2025-09-10T09:25:33.433042Z","shell.execute_reply":"2025-09-10T09:25:39.830108Z"}},"outputs":[{"name":"stdout","text":"Creating project directories...\n\n---\nRun the following command in your terminal to install dependencies:\n$ pip install pandas numpy scikit-learn xgboost catboost lightgbm tabpfn torch torchvision torchaudio matplotlib seaborn pyarrow fastparquet\n---\n\nChecking for CUDA support...\nCUDA Available: False\nWARNING: CUDA not found. Training will be on CPU.\nConfiguring logging...\n\nProject setup complete.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## Core Data Loading and Preprocessing\n\nThis section contains the functions responsible for interacting with the raw data. We begin by creating a robust data loader that handles the large parquet files, performs the critical ADC conversion to restore the data's dynamic range, and includes error handling. We then implement functions to apply the various calibration frames, such as dark subtraction and flat-field correction, to clean the instrumental signatures from the signal.","metadata":{}},{"cell_type":"code","source":"# ===================================================================\n# 2.1 & 2.2: Data Loading and Calibration\n# File: src/data_processing/loader.py\n# ===================================================================\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\n\n# --- 2.1: Data Loading Utilities ---\n\ndef load_adc_info(data_path):\n    \"\"\"Loads ADC conversion parameters.\"\"\"\n    return pd.read_csv(Path(data_path) / 'adc_info.csv').iloc[0]\n\ndef load_signal_data(file_path, adc_params, instrument):\n    \"\"\"Loads a single signal parquet file and applies ADC conversion.\"\"\"\n    try:\n        df = pd.read_parquet(file_path)\n        raw_signal = df.to_numpy()\n        \n        # Determine shape based on instrument\n        if instrument == 'FGS1':\n            reshaped_signal = raw_signal.reshape(-1, 32, 32)\n        elif instrument == 'AIRS-CH0':\n            reshaped_signal = raw_signal.reshape(-1, 32, 356)\n        else:\n            raise ValueError(\"Unknown instrument\")\n            \n        # Apply ADC conversion\n        gain = adc_params['gain']\n        offset = adc_params['offset']\n        signal_float64 = (reshaped_signal / gain + offset).astype(np.float64)\n        \n        logging.info(f\"Successfully loaded and converted {file_path}\")\n        return signal_float64\n    \n    except Exception as e:\n        logging.error(f\"Failed to load or process {file_path}: {e}\")\n        return None\n\n# --- 2.2: Calibration Data Processing ---\n\ndef load_calibration_files(planet_path, instrument, visit):\n    \"\"\"Loads all calibration files for a given instrument and visit.\"\"\"\n    calib_path = Path(planet_path) / f\"{instrument}_calibration_{visit}\"\n    calib_data = {}\n    for calib_type in ['dark', 'flat', 'dead', 'linear_corr', 'read']:\n        file_path = calib_path / f\"{calib_type}.parquet\"\n        if file_path.exists():\n            calib_data[calib_type] = pd.read_parquet(file_path).to_numpy()\n    return calib_data\n\ndef apply_calibrations(signal_data, calib_data):\n    \"\"\"Applies a simplified calibration pipeline.\"\"\"\n    # This is a simplified example. A real pipeline would be more complex.\n    processed_signal = signal_data\n    if 'dark' in calib_data:\n        processed_signal = processed_signal - calib_data['dark']\n    if 'flat' in calib_data:\n        # Avoid division by zero\n        flat = calib_data['flat']\n        flat[flat == 0] = 1\n        processed_signal = processed_signal / flat\n    \n    # Dead pixel masking could be applied here by setting values to NaN or interpolating\n    return processed_signal\n\n# --- 2.3: Multi-Visit Combination ---\n\ndef process_all_planet_visits(planet_path, adc_params):\n    \"\"\"\n    Loads all data for a single planet, handles multiple visits, \n    and applies calibrations.\n    \"\"\"\n    planet_path = Path(planet_path)\n    processed_data = {'FGS1': [], 'AIRS-CH0': []}\n\n    for instrument in ['FGS1', 'AIRS-CH0']:\n        visit_files = sorted(list(planet_path.glob(f'{instrument}_signal_*.parquet')))\n        \n        for visit_file in visit_files:\n            visit_id = visit_file.stem.split('_')[-1]\n            \n            # Load signal data\n            signal_data = load_signal_data(visit_file, adc_params, instrument)\n            if signal_data is None: continue\n            \n            # Load corresponding calibration data\n            calib_data = load_calibration_files(planet_path, instrument, visit_id)\n            \n            # Apply calibrations\n            calibrated_signal = apply_calibrations(signal_data, calib_data)\n            processed_data[instrument].append(calibrated_signal)\n            \n    # Combine visits by concatenating along the time axis\n    for instrument in processed_data:\n        if processed_data[instrument]:\n            processed_data[instrument] = np.concatenate(processed_data[instrument], axis=0)\n        else:\n            processed_data[instrument] = np.array([])\n            \n    return processed_data","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Comprehensive Feature Engineering Pipeline\n\nWith the data loaded and cleaned, we now focus on feature engineering. The strategy is to reduce the dimensionality of the vast time-series data into a compact and informative feature vector. We perform simple aperture photometry to create 1D light curves, extract basic statistical features from these light curves, and combine them with the scaled stellar parameters to form the final input for our models.","metadata":{}},{"cell_type":"code","source":"# ===================================================================\n# 3.1, 3.2 & 3.3: Feature Engineering\n# File: src/feature_engineering/builder.py\n# ===================================================================\nfrom sklearn.preprocessing import StandardScaler\n\ndef create_light_curve(signal_data):\n    \"\"\"\n    Creates a simplified 1D light curve from 3D signal data by summing\n    all pixel values for each timestamp. This is a basic form of photometry.\n    \"\"\"\n    if signal_data.ndim != 3:\n        return np.array([])\n    # Sum across the spatial dimensions (height and width)\n    return np.sum(signal_data, axis=(1, 2))\n\ndef extract_temporal_features(light_curve):\n    \"\"\"Extracts basic statistical features from a light curve.\"\"\"\n    if light_curve.size == 0:\n        return {'mean': 0, 'std': 0, 'min': 0, 'max': 0}\n    \n    return {\n        'mean': np.mean(light_curve),\n        'std': np.std(light_curve),\n        'min': np.min(light_curve),\n        'max': np.max(light_curve)\n    }\n\ndef get_stellar_features(planet_id, star_info_df):\n    \"\"\"Retrieves stellar parameters for a given planet_id.\"\"\"\n    return star_info_df[star_info_df['planet_id'] == planet_id].iloc[0]\n\ndef build_feature_vector(planet_id, all_calibrated_data, star_info_df):\n    \"\"\"\n    Builds a single feature vector for a planet by combining temporal\n    and stellar features.\n    \"\"\"\n    features = {'planet_id': planet_id}\n    \n    # Temporal features from light curves\n    for instrument in ['FGS1', 'AIRS-CH0']:\n        light_curve = create_light_curve(all_calibrated_data[instrument])\n        temp_features = extract_temporal_features(light_curve)\n        for key, val in temp_features.items():\n            features[f'{instrument}_{key}'] = val\n            \n    # Stellar features\n    stellar_params = get_stellar_features(planet_id, star_info_df)\n    features.update(stellar_params.to_dict())\n    \n    return features","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Baseline Models and Evaluation\n\nThis section establishes our modeling and evaluation framework. We implement the competition-specific Gaussian Log-Likelihood (GLL) metric, ensuring the heavy FGS1 channel weight is correctly applied. We then create a function to train a simple baseline model, such as Ridge regression, and a validation function to score its performance using a standard train-test split.","metadata":{}},{"cell_type":"code","source":"# ===================================================================\n# 4.1 & 4.2: Baseline Models and Evaluation\n# File: src/models/baseline.py and src/evaluation/metrics.py\n# ===================================================================\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import make_scorer\nimport sklearn.multioutput\n\n# --- 4.2: Evaluation Framework ---\n\ndef gll_score_single(y_true, y_pred_mean, y_pred_unc):\n    \"\"\"Calculates the Gaussian Log-Likelihood for a single prediction.\"\"\"\n    return -0.5 * (np.log(2 * np.pi) + np.log(y_pred_unc**2) + ((y_true - y_pred_mean)**2) / (y_pred_unc**2))\n\ndef calculate_weighted_gll(y_true, y_pred_mean, y_pred_unc, fgs1_weight=57.846):\n    \"\"\"Calculates the final weighted GLL score for the competition.\"\"\"\n    # Ensure inputs are numpy arrays\n    y_true = np.asarray(y_true)\n    y_pred_mean = np.asarray(y_pred_mean)\n    y_pred_unc = np.asarray(y_pred_unc)\n    \n    scores = gll_score_single(y_true, y_pred_mean, y_pred_unc)\n    \n    # Apply weights\n    weights = np.ones(y_true.shape[1])\n    weights[0] = fgs1_weight  # First column is FGS1\n    \n    weighted_scores = scores * weights\n    return np.sum(weighted_scores)\n\n\n# --- 4.1: Simple Regression Baselines ---\n\ndef train_and_evaluate_baseline(X, y):\n    \"\"\"Trains a Ridge Regressor and evaluates it using the GLL score.\"\"\"\n    \n    # Split data\n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n    \n    print(f\"Training on {X_train.shape[0]} samples, validating on {X_val.shape[0]} samples.\")\n    \n    # Initialize and train a simple model\n    # We use a MultiOutputRegressor to predict all 283 wavelengths at once\n    model = sklearn.multioutput.MultiOutputRegressor(Ridge(alpha=1.0))\n    model.fit(X_train, y_train)\n    \n    # Make predictions\n    y_pred_mean = model.predict(X_val)\n    \n    # Estimate uncertainty: A simple approach is to use the standard deviation\n    # of the training residuals as a constant uncertainty for all predictions.\n    train_residuals = y_train - model.predict(X_train)\n    y_pred_unc = np.std(train_residuals, axis=0)\n    \n    # Evaluate\n    score = calculate_weighted_gll(y_val, y_pred_mean, y_pred_unc)\n    print(f\"Validation Weighted GLL Score: {score:.4f}\")\n    \n    return model, score","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
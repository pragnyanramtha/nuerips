{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":99552,"databundleVersionId":13190393,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# NeurIPS : notebook by pragnyanramtha","metadata":{}},{"cell_type":"code","source":"# ===================================================================\n# 1.1: Project Setup and Dependencies\n# File: setup.py\n# ===================================================================\nimport os\nimport logging\nimport torch\nimport warnings\n\ndef setup_project_environment():\n    \"\"\"\n    Creates directories, installs packages, and configures logging.\n    \"\"\"\n    # --- Directory Creation ---\n    print(\"Creating project directories...\")\n    directories = ['data', 'notebooks', 'src/data_processing', 'src/models', \n                   'src/evaluation', 'src/utils', 'results/models', 'results/submissions']\n    for directory in directories:\n        os.makedirs(directory, exist_ok=True)\n    \n    # --- Package Installation ---\n    # In a real environment, you would run this in your terminal.\n    # We will list the command here for completeness.\n    print(\"\\n---\")\n    print(\"Run the following command in your terminal to install dependencies:\")\n    pip_install_command = (\"pip install pandas numpy scikit-learn xgboost catboost lightgbm \"\n                           \"tabpfn torch torchvision torchaudio matplotlib seaborn pyarrow fastparquet\")\n    print(f\"$ {pip_install_command}\")\n    print(\"---\\n\")\n\n    # --- CUDA Configuration ---\n    print(\"Checking for CUDA support...\")\n    is_cuda_available = torch.cuda.is_available()\n    print(f\"CUDA Available: {is_cuda_available}\")\n    if not is_cuda_available:\n        print(\"WARNING: CUDA not found. Training will be on CPU.\")\n    \n    # --- Logging and Warnings Configuration ---\n    print(\"Configuring logging...\")\n    logging.basicConfig(level=logging.INFO, \n                        format='%(asctime)s - %(levelname)s - %(message)s',\n                        filename='project_log.log',\n                        filemode='w')\n    \n    # Suppress common warnings for cleaner output\n    warnings.filterwarnings('ignore', category=FutureWarning)\n    \n    print(\"\\nProject setup complete.\")\n    logging.info(\"Project environment set up successfully.\")\n\n# Execute the setup\nsetup_project_environment()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Core Data Loading and Preprocessing\n\nThis section contains the functions responsible for interacting with the raw data. We begin by creating a robust data loader that handles the large parquet files, performs the critical ADC conversion to restore the data's dynamic range, and includes error handling. We then implement functions to apply the various calibration frames, such as dark subtraction and flat-field correction, to clean the instrumental signatures from the signal.","metadata":{}},{"cell_type":"code","source":"# ===================================================================\n# 2.1 & 2.2: Data Loading and Calibration\n# File: src/data_processing/loader.py\n# ===================================================================\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\n\n# --- 2.1: Data Loading Utilities ---\n\ndef load_adc_info(data_path):\n    \"\"\"Loads ADC conversion parameters.\"\"\"\n    return pd.read_csv(Path(data_path) / 'adc_info.csv').iloc[0]\n\ndef load_signal_data(file_path, adc_params, instrument):\n    \"\"\"Loads a single signal parquet file and applies ADC conversion.\"\"\"\n    try:\n        df = pd.read_parquet(file_path)\n        raw_signal = df.to_numpy()\n        \n        # Determine shape based on instrument\n        if instrument == 'FGS1':\n            reshaped_signal = raw_signal.reshape(-1, 32, 32)\n        elif instrument == 'AIRS-CH0':\n            reshaped_signal = raw_signal.reshape(-1, 32, 356)\n        else:\n            raise ValueError(\"Unknown instrument\")\n            \n        # Apply ADC conversion\n        gain = adc_params['gain']\n        offset = adc_params['offset']\n        signal_float64 = (reshaped_signal / gain + offset).astype(np.float64)\n        \n        logging.info(f\"Successfully loaded and converted {file_path}\")\n        return signal_float64\n    \n    except Exception as e:\n        logging.error(f\"Failed to load or process {file_path}: {e}\")\n        return None\n\n# --- 2.2: Calibration Data Processing ---\n\ndef load_calibration_files(planet_path, instrument, visit):\n    \"\"\"Loads all calibration files for a given instrument and visit.\"\"\"\n    calib_path = Path(planet_path) / f\"{instrument}_calibration_{visit}\"\n    calib_data = {}\n    for calib_type in ['dark', 'flat', 'dead', 'linear_corr', 'read']:\n        file_path = calib_path / f\"{calib_type}.parquet\"\n        if file_path.exists():\n            calib_data[calib_type] = pd.read_parquet(file_path).to_numpy()\n    return calib_data\n\ndef apply_calibrations(signal_data, calib_data):\n    \"\"\"Applies a simplified calibration pipeline.\"\"\"\n    # This is a simplified example. A real pipeline would be more complex.\n    processed_signal = signal_data\n    if 'dark' in calib_data:\n        processed_signal = processed_signal - calib_data['dark']\n    if 'flat' in calib_data:\n        # Avoid division by zero\n        flat = calib_data['flat']\n        flat[flat == 0] = 1\n        processed_signal = processed_signal / flat\n    \n    # Dead pixel masking could be applied here by setting values to NaN or interpolating\n    return processed_signal\n\n# --- 2.3: Multi-Visit Combination ---\n\ndef process_all_planet_visits(planet_path, adc_params):\n    \"\"\"\n    Loads all data for a single planet, handles multiple visits, \n    and applies calibrations.\n    \"\"\"\n    planet_path = Path(planet_path)\n    processed_data = {'FGS1': [], 'AIRS-CH0': []}\n\n    for instrument in ['FGS1', 'AIRS-CH0']:\n        visit_files = sorted(list(planet_path.glob(f'{instrument}_signal_*.parquet')))\n        \n        for visit_file in visit_files:\n            visit_id = visit_file.stem.split('_')[-1]\n            \n            # Load signal data\n            signal_data = load_signal_data(visit_file, adc_params, instrument)\n            if signal_data is None: continue\n            \n            # Load corresponding calibration data\n            calib_data = load_calibration_files(planet_path, instrument, visit_id)\n            \n            # Apply calibrations\n            calibrated_signal = apply_calibrations(signal_data, calib_data)\n            processed_data[instrument].append(calibrated_signal)\n            \n    # Combine visits by concatenating along the time axis\n    for instrument in processed_data:\n        if processed_data[instrument]:\n            processed_data[instrument] = np.concatenate(processed_data[instrument], axis=0)\n        else:\n            processed_data[instrument] = np.array([])\n            \n    return processed_data","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}